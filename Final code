import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict


class ExplorationEnhancedQLearning:
    def __init__(self, n_agents=5):
        self.q_table = np.zeros((64, 4))  # Shared Q-table
        self.n_agents = n_agents

        # Exploration mechanism related
        self.state_visit_count = np.zeros(64)  # Visit count for each state (global)
        self.episode_visit_count = np.zeros(64)  # Visit count for each state in the current episode
        self.state_first_visit_episode = {}  # Episode number when each state was first visited (not directly used, but good to keep for potential future use)
        self.exploration_bonus_decay = 0.99  # Exploration bonus decay rate
        self.current_episode = 0

        # Environment map
        self.env_map = np.array([
            ['S', 'F', 'F', 'F', 'F', 'H', 'F', 'F'],
            ['F', 'F', 'H', 'F', 'F', 'H', 'F', 'F'],
            ['F', 'F', 'W', 'H', 'F', 'H', 'F', 'F'],
            ['H', 'F', 'F', 'F', 'W', 'F', 'F', 'F'],
            ['F', 'F', 'F', 'H', 'F', 'F', 'H', 'F'],
            ['F', 'F', 'H', 'F', 'F', 'F', 'F', 'F'],
            ['W', 'F', 'F', 'F', 'H', 'H', 'F', 'H'],
            ['F', 'F', 'F', 'F', 'W', 'F', 'G', 'F']
        ])

        # Hyperparameters
        self.alpha = 0.1
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.9995

        # Exploration bonus parameters
        self.exploration_bonus_strength = 0.5  # Strength of exploration bonus
        self.novelty_threshold = 5  # States visited less than this count are considered "new"
        self.curiosity_bonus = 0.3  # Curiosity bonus for truly new states

        # Statistics
        self.successes = 0
        self.holes = 0
        self.walls = 0
        self.timeouts = 0
        self.episode_rewards = []
        self.episode_outcomes = []
        self.exploration_rewards = []  # Record exploration bonuses for analysis
        self.success_rate_history = []  # To store success rate for plotting
        self.avg_reward_history = []  # To store average reward for plotting

        print(f"Initialization complete! {n_agents} agents learning in parallel + Exploration Enhancement")
        print("Reward Mechanism:")
        print("- Reaching Goal: +10.0")
        print("- Falling into Hole: -1.0")
        print("- Hitting Wall: -0.5")
        print("- Distance Reward: 0 to 0.1")
        print("- Step Penalty: -0.02")
        print("üîç Exploration Enhancement Mechanism:")
        print(f"- New State Bonus: +{self.exploration_bonus_strength}")
        print(f"- Curiosity Bonus: +{self.curiosity_bonus}")
        print(f"- Rare State Bonus: Dynamic bonus based on visit frequency")

    def get_exploration_bonus(self, state):
        """Calculates the exploration bonus"""
        total_bonus = 0.0

        # 1. First visit bonus (first visit within this episode)
        if self.episode_visit_count[state] == 0:
            first_visit_bonus = self.exploration_bonus_strength
            total_bonus += first_visit_bonus

            # If it's a global first visit, add an extra curiosity bonus
            if self.state_visit_count[state] == 0:
                total_bonus += self.curiosity_bonus
                # print(f"üÜï Discovered new state {state}! Bonus: +{self.curiosity_bonus}") # Commented for less verbose output

        # 2. Rare state bonus (higher bonus for less visited states)
        if self.state_visit_count[state] < self.novelty_threshold:
            rarity_bonus = self.exploration_bonus_strength / (1 + self.state_visit_count[state])
            total_bonus += rarity_bonus

        # 3. Time-decaying exploration bonus
        decay_factor = (self.exploration_bonus_decay ** self.current_episode)
        total_bonus *= max(0.1, decay_factor)  # Minimum 10% of the exploration bonus maintained

        return total_bonus

    def get_distance_reward(self, state):
        """Calculates the distance reward"""
        row, col = divmod(state, 8)
        goal_row, goal_col = 7, 6  # Goal is 'G' at (7,6)

        distance = abs(row - goal_row) + abs(col - goal_col)
        max_distance = 13  # Max distance from (0,0) to (7,6) is 7+6=13

        distance_reward = (max_distance - distance) / max_distance * 0.1
        return distance_reward

    def simulate_step(self, state, action):
        """Simulates environment step"""
        row, col = divmod(state, 8)
        moves = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # LEFT, DOWN, RIGHT, UP

        if action < len(moves):
            drow, dcol = moves[action]
            new_row, new_col = row + drow, col + dcol

            if not (0 <= new_row < 8 and 0 <= new_col < 8):
                return state, -0.9, False, "wall"

            new_state = new_row * 8 + new_col
            cell = self.env_map[new_row, new_col]

            if cell == 'H':
                return new_state, -1.0, True, "hole"
            elif cell == 'G':
                return new_state, 10.0, True, "goal"  # Keep strong goal reward
            elif cell == 'W':
                return state, -0.5, False, "wall"
            else:
                return new_state, 0.0, False, "move"

        return state, -0.5, False, "error"  # Should not happen

    def get_total_reward(self, state, action, next_state, env_reward, done, outcome):
        """Calculates total reward (including exploration bonus)"""
        total_reward = env_reward

        # Distance reward
        if outcome == "move" or outcome == "goal":
            distance_reward = self.get_distance_reward(next_state)
            total_reward += distance_reward

        # Exploration reward (only given in safe states)
        if outcome == "move" or outcome == "goal":
            exploration_reward = self.get_exploration_bonus(next_state)
            total_reward += exploration_reward
            

            # Record exploration reward for analysis
            if exploration_reward > 0:
                self.exploration_rewards.append(exploration_reward)

        # Step penalty
        if not done or outcome == "goal":  # Apply penalty unless it's a hole/wall
            step_penalty = -0.02
            total_reward += step_penalty

        return total_reward

    def choose_action_with_curiosity(self, state):
        """Action selection with curiosity"""
        if np.random.rand() < self.epsilon:
            # During exploration, tend to choose actions leading to unvisited or less visited states
            action_values = []

            for action in range(4):
                # Simulate the result of this action
                next_state, _, _, outcome = self.simulate_step(state, action)

                if outcome != "hole" and outcome != "wall":
                    # Calculate novelty of this state
                    novelty = 1.0 / (1 + self.state_visit_count[next_state])
                    action_values.append((action, novelty))
                else:
                    action_values.append((action, 0))  # Dangerous actions have 0 novelty

            # Probabilistic selection based on novelty
            if action_values:
                actions, novelties = zip(*action_values)
                novelties = np.array(novelties)

                if np.sum(novelties) > 0:
                    probabilities = novelties / np.sum(novelties)
                    return np.random.choice(actions, p=probabilities)

            return np.random.choice(4)  # Fallback to random if no safe/novel actions
        else:
            return np.argmax(self.q_table[state])

    def train_episode_batch(self):
        """Trains a batch of episodes"""
        batch_outcomes = []
        batch_rewards = []

        for agent_id in range(self.n_agents):
            # Reset visit counts for the current episode
            self.episode_visit_count.fill(0)

            state = 0
            total_reward = 0
            steps = 0
            max_steps = 200
            done = False

            # Record new states visited in this episode
            new_states_this_episode = set()

            while not done and steps < max_steps:
                action = self.choose_action_with_curiosity(state)
                next_state, env_reward, done, outcome = self.simulate_step(state, action)

                # Update visit counts
                self.episode_visit_count[next_state] += 1
                self.state_visit_count[next_state] += 1

                # Record new states
                if self.state_visit_count[next_state] == 1:
                    new_states_this_episode.add(next_state)

                # Calculate total reward
                total_reward_step = self.get_total_reward(state, action, next_state, env_reward, done, outcome)
                total_reward += total_reward_step

                # Q-learning update
                if done:
                    target = total_reward_step
                else:
                    target = total_reward_step + self.gamma * np.max(self.q_table[next_state])

                old_q = self.q_table[state, action]
                self.q_table[state, action] += self.alpha * (target - old_q)

                state = next_state
                steps += 1

                if done:
                    if outcome == "goal":
                        self.successes += 1
                        # print(
                        #     f"üéâ Agent {agent_id} Success! Steps: {steps}, Reward: {total_reward:.3f}, New States: {len(new_states_this_episode)}") # Commented for less verbose output
                    elif outcome == "hole":
                        self.holes += 1
                    break

            if not done:
                self.timeouts += 1
                outcome = "timeout"

            # Add extra bonus for episodes that discover many new states
            if len(new_states_this_episode) >= 3:
                exploration_bonus_for_episode = len(new_states_this_episode) * 0.1
                total_reward += exploration_bonus_for_episode
                # print(
                #     f"üîç Agent {agent_id} Exploration Bonus: Discovered {len(new_states_this_episode)} new states, extra +{exploration_bonus_for_episode:.2f}") # Commented for less verbose output

            batch_outcomes.append(outcome)
            batch_rewards.append(total_reward)
            self.episode_rewards.append(total_reward)
            self.episode_outcomes.append(outcome)

        self.current_episode += 1
        return batch_outcomes, batch_rewards

    def print_exploration_stats(self):
        """Prints exploration statistics"""
        visited_states = np.count_nonzero(self.state_visit_count)
        total_states = 64
        safe_states = np.sum(self.env_map != 'H')  # Number of non-hole states

        print(f"\nüîç Exploration Statistics:")
        print(f"  Visited States: {visited_states}/{total_states} ({visited_states / total_states:.1%})")
        print(f"  Safe State Coverage: {visited_states}/{safe_states} ({visited_states / safe_states:.1%})")

        # Visit frequency analysis
        visit_counts = self.state_visit_count[self.state_visit_count > 0]
        if len(visit_counts) > 0:
            print(f"  Average Visit Count: {np.mean(visit_counts):.1f}")
            print(f"  Visit Count Range: {np.min(visit_counts):.0f} - {np.max(visit_counts):.0f}")

        # Exploration reward statistics
        if self.exploration_rewards:
            print(f"  Total Exploration Bonuses Granted: {len(self.exploration_rewards)}")
            print(f"  Average Exploration Bonus: {np.mean(self.exploration_rewards):.3f}")

    def print_progress(self, episode, total_episodes):
        """Prints training progress"""
        total_attempts = len(self.episode_outcomes)

        if total_attempts > 0:
            success_count = self.episode_outcomes.count("goal")
            hole_count = self.episode_outcomes.count("hole")
            timeout_count = self.episode_outcomes.count("timeout")

            success_rate = success_count / total_attempts
            hole_rate = hole_count / total_attempts
            timeout_rate = timeout_count / total_attempts
        else:
            success_rate = hole_rate = timeout_rate = 0

        recent_rewards = self.episode_rewards[-100:] if len(self.episode_rewards) >= 100 else self.episode_rewards
        avg_reward = np.mean(recent_rewards) if recent_rewards else 0

        non_zero_q = np.count_nonzero(self.q_table)
        max_q = np.max(self.q_table)
        min_q = np.min(self.q_table)

        print(f"\nEpisode {episode}/{total_episodes}:")
        print(f"  Successes: {success_count} ({success_rate:.2%})")
        print(f"  Holes: {hole_count} ({hole_rate:.2%})")
        print(f"  Timeouts: {timeout_count} ({timeout_rate:.2%})")
        print(f"  Average Reward (last 100 episodes): {avg_reward:.3f}")
        print(f"  Epsilon: {self.epsilon:.3f}")
        print(f"  Q-value Range: [{min_q:.3f}, {max_q:.3f}]")

        # Add exploration statistics
        self.print_exploration_stats()

        # Store for plotting
        self.success_rate_history.append(success_rate)
        self.avg_reward_history.append(avg_reward)

    def train(self, episodes=2000):
        """Main training loop"""
        print(f"Starting training for {episodes} episodes...")

        first_success_episode = None

        # Calculate print interval based on total episodes
        print_interval = max(1, episodes // 20)  # Print roughly 20 times during training
        if episodes < 200:
            print_interval = max(1, episodes // 5)

        for episode in range(episodes):
            outcomes, rewards = self.train_episode_batch()

            if "goal" in outcomes and first_success_episode is None:
                # Check for goal outcomes across all agents in the batch
                if any(o == "goal" for o in outcomes):
                    first_success_episode = episode
                    print(f"üéâ First success achieved in Episode {episode}")

            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

            if episode % print_interval == 0 or episode == episodes - 1:
                self.print_progress(episode, episodes)

        print(f"\nTraining complete!")
        if first_success_episode is not None:
            print(f"First success occurred in Episode {first_success_episode}")

        # Final exploration statistics
        self.print_exploration_stats()

    def test_learned_policy(self, n_tests=20):
        """Tests the learned policy"""
        print(f"\n=== Testing Learned Policy ({n_tests} times) ===")

        successes = 0
        for test in range(n_tests):
            state = 0
            path = [state]
            done = False
            steps = 0

            while not done and steps < 100:  # Max steps for test run
                action = np.argmax(self.q_table[state])
                next_state, env_reward, done, outcome = self.simulate_step(state, action)
                path.append(next_state)
                state = next_state
                steps += 1

                if done:
                    if outcome == "goal":
                        successes += 1
                        # print(f"‚úÖ Test {test + 1}: Success! Steps: {steps}") # Commented for less verbose output
                    else:
                        # print(f"‚ùå Test {test + 1}: {outcome}") # Commented for less verbose output
                        pass
                    break
            else:
                # print(f"‚è∞ Test {test + 1}: Timeout") # Commented for less verbose output
                pass

        success_rate = successes / n_tests
        print(f"\nüéØ Final Test Success Rate: {successes}/{n_tests} = {success_rate:.1%}")

        return success_rate

    def plot_results(self):
        """Plots the training results (rewards and success rate)."""
        episodes_run = len(self.episode_rewards) // self.n_agents  # Total episodes considering batches
        x_axis_points = np.arange(0, episodes_run, max(1, episodes_run // len(self.avg_reward_history)))
        # Adjust x_axis_points to match the length of self.avg_reward_history
        if len(x_axis_points) > len(self.avg_reward_history):
            x_axis_points = x_axis_points[:len(self.avg_reward_history)]
        elif len(x_axis_points) < len(self.avg_reward_history):
            # If for some reason history is longer (e.g., small episodes, frequent prints)
            # just use an evenly spaced range for the history length
            x_axis_points = np.linspace(0, episodes_run, len(self.avg_reward_history))

        plt.figure(figsize=(14, 6))

        # Plot Average Rewards
        plt.subplot(1, 2, 1)
        plt.plot(x_axis_points, self.avg_reward_history, label='Average Reward (last 100 episodes)')
        plt.title('Training Rewards Over Episodes')
        plt.xlabel('Episodes')
        plt.ylabel('Average Reward')
        plt.grid(True)
        plt.legend()

        # Plot Success Rate
        plt.subplot(1, 2, 2)
        plt.plot(x_axis_points, np.array(self.success_rate_history) * 100, label='Success Rate (%)')
        plt.title('Training Success Rate Over Episodes')
        plt.xlabel('Episodes')
        plt.ylabel('Success Rate (%)')
        plt.grid(True)
        plt.legend()

        plt.tight_layout()
        plt.show()

    def plot_q_table_and_policy(self):
        """Visualizes the learned policy and Q-values on the grid."""
        fig, ax = plt.subplots(1, 2, figsize=(16, 8))

        # Helper to convert state index to (row, col)
        def state_to_coord(state_idx):
            return divmod(state_idx, 8)

        # Plot Environment Map
        cmap = plt.cm.get_cmap('viridis', 5)  # For S, F, W, H, G
        norm = plt.Normalize(vmin=0, vmax=4)

        # Mapping for cell types to numerical values for colormap
        cell_type_to_int = {'S': 0, 'F': 1, 'W': 2, 'H': 3, 'G': 4}
        grid_numeric = np.zeros(self.env_map.shape)
        for r in range(8):
            for c in range(8):
                grid_numeric[r, c] = cell_type_to_int.get(self.env_map[r, c], 1)  # Default F

        im = ax[0].imshow(grid_numeric, cmap=cmap, norm=norm, origin='upper', extent=[0, 8, 8, 0])

        # Colorbar with labels
        cbar = fig.colorbar(im, ax=ax[0], ticks=[0.4, 1.2, 2, 2.8, 3.6])  # Adjust ticks to center on colors
        cbar.ax.set_yticklabels(['Start (S)', 'Frozen (F)', 'Wall (W)', 'Hole (H)', 'Goal (G)'])

        ax[0].set_xticks(np.arange(0.5, 8.5, 1), minor=False)
        ax[0].set_yticks(np.arange(0.5, 8.5, 1), minor=False)
        ax[0].set_xticklabels([])
        ax[0].set_yticklabels([])
        ax[0].grid(which='major', color='black', linestyle='-', linewidth=2)

        # Overlay policy arrows and Q-values
        arrow_map = {0: '‚Üê', 1: '‚Üì', 2: '‚Üí', 3: '‚Üë'}  # LEFT, DOWN, RIGHT, UP
        for state in range(64):
            r, c = state_to_coord(state)
            cell_type = self.env_map[r, c]

            if cell_type in ['F', 'S', 'G']:  # Only show policy for traversable states
                best_action = np.argmax(self.q_table[state])
                arrow = arrow_map[best_action]
                ax[0].text(c + 0.5, r + 0.5, arrow, ha='center', va='center',
                           color='white', fontsize=20, fontweight='bold')

            # Display Q-value for the best action (or overall max Q-value for the state)
            max_q_val = np.max(self.q_table[state])
            if max_q_val != 0:  # Only show non-zero Q-values
                ax[0].text(c + 0.5, r + 0.1, f'{max_q_val:.1f}', ha='center', va='top', color='purple', fontsize=8)

        ax[0].set_title('Learned Policy (Arrows: Greedy Action, Numbers: Max Q-value)')

        # Plot State Visit Counts Heatmap
        visit_counts_reshaped = self.state_visit_count.reshape((8, 8))
        im2 = ax[1].imshow(visit_counts_reshaped, cmap='hot_r', origin='upper', extent=[0, 8, 8, 0])
        fig.colorbar(im2, ax=ax[1], label='Total State Visit Count')

        ax[1].set_xticks(np.arange(0.5, 8.5, 1), minor=False)
        ax[1].set_yticks(np.arange(0.5, 8.5, 1), minor=False)
        ax[1].set_xticklabels([])
        ax[1].set_yticklabels([])
        ax[1].grid(which='major', color='black', linestyle='-', linewidth=2)

        for r in range(8):
            for c in range(8):
                state_idx = r * 8 + c
                ax[1].text(c + 0.5, r + 0.5, f'{self.state_visit_count[state_idx]:.0f}', ha='center', va='center',
                           color='green' if self.state_visit_count[state_idx] > 0 else 'gray', fontsize=9)

        ax[1].set_title('State Visit Counts Heatmap')

        plt.tight_layout()
        plt.show()


def main():
    print("=== Multi-Agent Q-Learning with Exploration Enhancement ===")

    agent = ExplorationEnhancedQLearning(n_agents=5)
    agent.train(episodes=3000)

    # Test the policy after training
    test_success_rate = agent.test_learned_policy(n_tests=100)  # Increased test runs for more reliable rate

    # Generate visualizations
    agent.plot_results()
    agent.plot_q_table_and_policy()


if __name__ == "__main__":
    main()

